# -*- coding: utf-8 -*-
"""PROJECT GRP 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w9kHl1R1wajqjjJjRLsZx3CL1AHXYSZr

**ANALYSIS OF THE PRICE VOLATILITY AND THE FACTORS THAT BRING ABOUT PRICE FLUCTUATIONS.**

**DATA PREPARATIONS**

**1) we start by importing the relevant libraries**
"""
#importing streamlit
import streamlit as st
# importing pandas
import pandas as pd

# importing numpy
import numpy as np

# importing datetime
import datetime as dt
"""**2) Loading our datasets**"""
st.title("Group 3 final project")
# Loading data on food production per capita in  kenya
dfc = pd.read_csv('/content/GDP.csv', delimiter=',')
dfc.head(8)

# Loading data on historical CPI series
historical_cpi_series = pd.read_excel(
    '/content/Historical CPI series 2020.xlsx')
historical_cpi_series.head()

# Loading data on food production per capital
kenya_food_security = pd.read_csv(
    '/content/food-security-in-kenya-2000-2019-suite-of-food-security-indicators-faostat-per-capita-food-produ.csv'
)
kenya_food_security.head()

# Loading data on climate
climate = pd.read_csv(
    '/content/observed-average-annual-precipitation-of-kenya-for-1901-2020.csv'
)
climate.head()

# Loading data on food prices
df = pd.read_csv('/content/wfp_food_prices_ken.csv')
df.head(8)
"""**DATA CLEANING**

**CLEANING DATA FOR THE HISTORICAL CPI DATA**
"""

# Cleaning the CPI data
# Checking Validity
# Renaming column names
historical_cpi_series.rename(columns={
    'MONTHLY CPI FROM 1962 ': 'year',
    'Unnamed: 1': 'Month',
    'Unnamed: 2': 'CPI'
},
                             inplace=True)

#Checking completeness
# Finding null values and visualizing null values per column in the data set
historical_cpi_series.isnull().describe()
# Filling missing values for year with the foward fill technique
historical_cpi_series['year'] = historical_cpi_series['year'].fillna(
    method='ffill')
# Checking for any reamaining null values
historical_cpi_series.isnull().describe()

# Checking Validity
# Creating a new column day with a arbitrary value 01
historical_cpi_series['Day'] = '01'
# Dropping the first two rows with irrellevant data
historical_cpi_series = historical_cpi_series.drop(
    [historical_cpi_series.index[0], historical_cpi_series.index[1]])
# Visualizing the month column
historical_cpi_series['Month'].unique()
# Replacing the months with appropiate numerical reprsentations
historical_cpi_series = historical_cpi_series.replace([
    'Mar', 'Jun', 'Sep', 'Dec', 'Jan', 'Feb', 'Apr', 'May', 'Jul', 'Aug',
    'Oct', 'Nov'
], ['03', '06', '09', '12', '01', '02', '04', '05', '07', '08', '10', '11'])
# Converting year column from object to integer
historical_cpi_series['year'] = historical_cpi_series['year'].astype(int)
# Dropping all data of the years before 2006
historical_cpi_series = historical_cpi_series[
    historical_cpi_series['year'] >= 2006]
# Converting year column from integer to string
historical_cpi_series['year'] = historical_cpi_series['year'].astype(str)
# converting CPI column from integer to str
historical_cpi_series['CPI'] = historical_cpi_series['CPI'].astype(str)
# Joining the year month and day columns to form a new column date
historical_cpi_series['Date'] = historical_cpi_series[['year', 'Month',
                                                       'Day']].agg('-'.join,
                                                                   axis=1)
# Converting the column date from object to date time
historical_cpi_series['Date'] = pd.to_datetime(historical_cpi_series['Date'])
# Drop the date and month column
historical_cpi_series = historical_cpi_series.drop(['Month', 'Day'], axis=1)
# Convert CPI Column to integer
historical_cpi_series['CPI'] = historical_cpi_series['CPI'].astype(float)
# visualizing the cleaned data and getting data types
historical_cpi_series.info()
"""Checking completeness;
- The historical CPI Series was queried for null values
- The null values were studied observed only on the year column.
- The empty spaces in year column was due to data input format that inserted the one year and the months for that year the data was collected leaving many spaces.
- The null values were filled using the foward fill technique
- No further null values were observed after this step

Checking data validity;
- The column names containing data on year, month and CPI   were renamed appropiately to year, month and CPI
- A new column day was added with an arbitrary value 01, for later joining to form new column date
- All the columns that weren't in string format were converted to strings
- The year, month and day column were merged to form form a new column date
- The date column was then converted to date time format
- The year column was then converted to integer and all dates lower than 2006 were dropped 
- The CPI data was then reconverted to float value

"""

# checking for duplicates using the duplicated method
historical_cpi_series.duplicated().any()
"""There were no duplicate datasets in the dataframe containing CPI information"""

# getting Average CPI per year(Group by year the CPI and find mean)
average_cpi_per_year = historical_cpi_series['CPI'].groupby(
    historical_cpi_series['year']).mean().reset_index()
# Creating a dataframe from the grouped CPI's
average_cpi_per_year = pd.DataFrame(average_cpi_per_year)
# Visualizing the newly grouped table
average_cpi_per_year
#Renaming the column name from CPI to average_cpi_per_year
average_cpi_per_year.rename(columns={'CPI': 'av CPI per year'}).head(1)
average_cpi_per_year.head()
"""## Kenyan commodity prices data cleaning"""

# we now have to check the columnns of the dataset with missing values

df.isnull().any()
# To understand how many distinct dates are involved in this dataset

df['date'].unique()
# we can preview the dataset in terms of its properties to see whether we can change some properties or not

df.describe()
# As per our data understanding ,we found three data columns to be not relevant to the research problem.
# we will drop them since we will not need them in our analysis
# Columns like latitude, longitude and priceflag were dropped because they are not ralevant to our research problem.
df.drop(["latitude", "longitude", "priceflag"], axis=1, inplace=True)
# dropping the first row

df = df.drop([0], axis=0)
# We have to check the accuracy of the dataset in terms of the right datatype for each column
df['date'] = pd.to_datetime(df['date'])
formatted_df = df['date'].dt.strftime("%m/%d/%y")
# Change the datatype to the right datatype
df["price"] = df["price"].astype(float)
df["usdprice"] = df["usdprice"].astype(float)
df.rename({
    'admin1': 'Regions',
    'admin2': 'Sub_region'
},
          axis='columns',
          inplace=True)
df.duplicated().any
# Splitting the date column to day month year
df['year'] = pd.DatetimeIndex(df['date']).year
df['month'] = pd.DatetimeIndex(df['date']).month
df['day'] = pd.DatetimeIndex(df['date']).day
df.head()
# Grouped datasets
df.columns
df['grouped'] = 1  #initially, set that counter to 1.
group_data = df.groupby([
    'Regions', 'Sub_region', 'market', 'category', 'commodity', 'unit', 'year',
    'pricetype', 'currency'
])['price'].mean().reset_index()  #sum function
group_data
commodity_prices = pd.DataFrame(group_data)
commodity_prices.sort_values(by='year', ascending=True).head()
#commodity_prices.info()
# Convert year to float
commodity_prices['year'] = commodity_prices['year'].astype(str)
commodity_prices.head(20)
"""## Rainfall data Datacleaning"""

# Creating dataframe with desired years
climate_kenya = climate[climate['Category'] > 2005].reset_index()
climate_kenya.head()
# To confirm DataFrame dimensions
climate_kenya.shape

# Checking for missing values
climate_kenya.isnull().any()
climate_kenya.head(10)

#Removing white spaces on column name and changing to lower case
climate_kenya.columns = climate_kenya.columns.str.strip().str.lower(
).str.replace(' ', '_').str.replace('(', '').str.replace(')', '')
climate_kenya.head()


# Creating a function to drop unnecessary columns
def dropping_columns(data, cols):
    data.drop(cols, axis=1, inplace=True)

    return data


columns = ['index', '5-yr_smooth']
climate_kenya = dropping_columns(climate_kenya, columns)

# checking for duplicates
climate_kenya.duplicated().any()

# Renaming the column category to year
climate_kenya.rename({
    'category': 'year',
    'annual_mean': 'annual_rainfall'
},
                     axis=1,
                     inplace=True)
climate_kenya.head()
# Convert year to float
climate_kenya['year'] = climate_kenya['year'].astype(str)
climate_kenya.head(10)
"""## Consumer production per capita"""

# CLEANING

# 1. Validity
# procedure: removing irrelevant observations
# data cleaning action: removing 2001 - 2005 rows
# explanation: they're not relevant to our study

food = kenya_food_security.drop([0, 1, 2, 3, 4])
food
# resetting index
# food_security = food.reset_index()

# dropping index
food.reset_index(drop=True, inplace=True)

# 2. Completeness
# procedure: check for missing values
# data cleaning action: check for missing values as a whole
# explanation: missing values result to incomplete analysis

food.isnull()

# 4. Consistency
# *** no duplicates observed ***

# 3. Uniformity
# changing 'Year' to lower

food.rename(columns={'Year': 'year'}, inplace=True)

# procedure: Starndadization - data types
# data cleaning action: changing yeat to datetime
# explanation: all the other datasets have year as datetime because we'll merge

# checking data types
food.dtypes

# changing year to datetime
# food_security['Year'] = food_security['Year'].astype('datetime64[ns]')
# df['Date'] = df['Date'].astype('datetime64[ns]')

# checking data types again
food.dtypes
# Convert year to float
food['year'] = food['year'].astype(str)

food.head(10)
"""## GDP growth data cleaning"""

#To get the data from the country name Kenya

df_kenya = dfc[dfc['Country Name'] == "Kenya"]
df_kenya

# # To preview which columns are null

df_kenya.isnull()

# # lets understand the datatype

#df_kenya.info()

# # lets get the properties of the data

df_kenya.describe()

# # We drop column that wont be used to analyse the business objectives

#df_kenya.drop(df_kenya.loc[:,'1960':'2005'].columns, axis = 1, inplace = True)

#df_kenya.head()

# # we now transpose the dataset we want

df_kenya = df_kenya.transpose()
df_kenya
# # converting to CSV file
# df_kenya.to_csv("Annual_GDP_growth.csv")

# # reseting the index

df_kenya = df_kenya.reset_index()
df_kenya

# # we need to drop the first colums and rename the column names

df_kenya = df_kenya.drop([0, 1, 3], axis=0)
df_kenya

# # resets the index

df_kenya = df_kenya.reset_index()
df_kenya

# # Drop the level_o column(last index created)

df_kenya.drop(["level_0"], axis=1, inplace=True)

df_kenya.head()

# # Renaming the column names

df_kenya.rename({
    'index': 'year',
    121: 'GDP_growth_annual%'
},
                axis='columns',
                inplace=True)
df_kenya

# # we need to drop the first colums and rename the column names

df_kenya = df_kenya.drop([0], axis=0)
df_kenya

# # reseting the index

df_kenya = df_kenya.reset_index()
df_kenya

# # Drop the new index again
df_kenya.drop(["index"], axis=1, inplace=True)

df_kenya

# # Lets preview the data we have

# Convert year to float
df_kenya['GDP_growth_annual%'] = df_kenya['GDP_growth_annual%'].astype(float)
df_kenya['GDP_growth_annual%'] = df_kenya['GDP_growth_annual%'] / 100
# Convert year to float
df_kenya['year'] = df_kenya['year'].astype(int)
df_kenya.info()
# Drop values below 2006
df_kenya = df_kenya[df_kenya['year'] > 2005]
df_kenya.head()
df_kenya.reset_index(drop=True, inplace=True)
df_kenya.head()
df_kenya['year'] = df_kenya['year'].astype(str)
df_kenya.info()
df_kenya.head(10)
"""## Merge frames"""

# We endeavour to merge all of the cleaned datasets into one unified data pool

average_cpi_per_year.columns
df10 = average_cpi_per_year.merge(df_kenya, how='left', on='year')
df10

df11 = df10.merge(climate_kenya, how='left', on='year')
df11
df12 = df11.merge(food, how='left', on='year')
df12
df13 = df12.merge(commodity_prices, how='left', on='year')
df13.info()
df13.head(10)
"""## Exporting data"""

# Exporting data one
from google.colab import files

df13.to_csv('project_analysis.csv')
files.download('project_analysis.csv')

# Exporting data 2
df13.to_csv("Project_Analysis.csv")
df13

df13.head()
"""**ANALYSIS OF THE CORRELATION BETWEEN THE CORRELATION OF INFLATION AND FOOD PRICE VOLATILITY.**"""

# First we want to extract the relevant data we will use for the analysis of CPI on the relevant commodity price flactuation

five_minutes_lol = df13.groupby(['year', 'CPI'])['price'].mean()
five_minutes_lol.head(30)

# We reset the index so that we can hace a structured dataset for plotting and what nots

five_minutes_lol = df13.groupby(['year', 'CPI', 'commodity'])['price'].mean()
df14 = five_minutes_lol.reset_index()
df14.head(30)

df15 = df14[df14['commodity'] == 'Maize (white)']
df15

import pandas as pd
import numpy as np
import pandas_datareader as pdr
import matplotlib.pyplot as plt
from matplotlib.widgets import Button
from matplotlib import rcParams

rcParams['figure.figsize'] = (10, 8)
plt.plot(df14.CPI, 'c-')
# plt.plot(df14.price	, 'r-')
# plt.grid(True, color = 'k', linestyle = ':')
plt.legend()

import pandas as pd
import numpy as np
import pandas_datareader as pdr
import matplotlib.pyplot as plt
from matplotlib.widgets import Button
from matplotlib import rcParams

rcParams['figure.figsize'] = (6, 6)
plt.plot(df15.CPI, 'c-')
plt.plot(df15.price, 'r-')
plt.grid(True, color='k', linestyle=':')
plt.title('Line graph of inflation and maize(white) prices')
xlabel = df15.year
plt.legend()
